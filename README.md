# GENERATIVE-TEXT-MODEL
*COMPANY*: CODTECH IT SOLUTIONS

*NAME*: TUSHAR SUHAGPURE 

*INTER ID*: CT06DR1876

*DOMAIN*: ARTIFICIAL INTELLIGENCE

*DURATION*: 6 WEEKS

*MENTOR*: NEELA SANTOSH

##(Internship Task 4 description)
# Task 4 â€“ Generative Text Model 

## Project Overview
This project is completed as **Task 4** of my AI internship at Codtech.  
The objective of this task is to create a **text generation model** using a pretrained GPT-2 model to generate coherent paragraphs based on user prompts.

Neural text generation models like GPT-2 are capable of producing human-like text, making them useful for applications such as chatbots, content creation, and more.

## Objective of Task 4
The main goals of this task are:
- To understand how generative language models work.
- To utilize a pretrained GPT-2 model for text generation.
- To demonstrate generating coherent and contextually relevant text from specific prompts.
- To showcase the practical use of AI in natural language processing.

## Model and Technique Used
- **Model Used:** GPT-2 (Generative Pre-trained Transformer 2)
- **Framework:** Hugging Face Transformers
- **Why GPT-2?**
  - Pretrained on large-scale text data.
  - Capable of generating high-quality text.
  - Easy to use with minimal setup.

This model is used for **inference** only, and no additional training is required.

## Tools & Technologies Used
- **Operating System:** macOS
- **Programming Language:** Python
- **Code Editor:** Visual Studio Code (VS Code)
- **Libraries Used:**
  - "torch"
  - "transformers"

These libraries are essential for loading and utilizing the GPT-2 model.

### File Description:
- **text_generator.py**  
  Python script that loads the GPT-2 model and generates text based on user prompts.
